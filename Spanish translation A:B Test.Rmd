---
title: "Spanish translation A/B Test"
author: "Zhongyuan Zhang"
date: "1/25/2020"
output: html_document
---

Goals:

Confirm that the test is actually negative. That is, it appears that the old version of the site with just one translation across Spain and LatAm performs better

Explain why that might be happening. Are the localized translations really worse?

```{r}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(psych)
```

```{r}
#read data
user<-read.csv("user_table.csv",stringsAsFactors = FALSE, header= TRUE)
test<-read.csv("test_table.csv",stringsAsFactors = FALSE, header= TRUE)

#check duplication
identical(unique(user$user_id),user$user_id)# TRUE meaning no duplicated data

identical(unique(test$user_id),test$user_id)# TRUE meaning no duplicated data
```

```{r}
# Is everyone in one table also in the other one?
dim(user)[1]-dim(test)[1] # the answer is no
```
Looks like the user table is busted and we have some users ids missing. Therefore when merging the two tables, i have to be careful to do not lose the user ids in the test table, but not in the user table.

```{r}
data<-left_join(test,user,by="user_id") # no data is losing in this way

str(data)

data$date<-as.Date(data$date)

summary(data)
describe(data)
```

First question is: check test results. But even before that, let's make sure it is true Spain converts much better than the rest of the LatAm

```{r}
#before the test
data_convertion_country<-data%>%
  group_by(country)%>%
  summarise(conversion_rate=mean(conversion[test==0]))%>%
  arrange(desc(conversion_rate))

head(data_convertion_country)

#indeed, Spain has the top conversion rate, better than the rest of the LatAm
```
```{r}
# a simple t-test here should work. We have collected ~0.5MM data and test/control split is ~50/50.

data_test<-subset(data,country!="Spain") # nothing new has been added to the service in Spain, so no point in keeping those users

t.test(data_test$conversion[data_test$test==1],data_test$conversion[data_test$test==0]) # in non-spain contries there's statistical significance in their mean differences between test group and control group; control group has better performance than the tested group,meaning that the change has a negative impact on the conversion-
```

Not in the test are converting at 4.8% while users in the test just at 4.3%. That's a 10% drop, which would be dramatic if it were true. The most likely reason for weird A/B test results are:
1. We didn't collect enough data.
2. Some bias has been introduced in the experiment so that test/ control people are not really random


In data science, whenever results appear too bad or too good  to be true, they are not true.

Firstly, let's plot day by day, to see if these weird results have been constantly happening or they just started happening all of a sudden
```{r}
data_test_by_day<-data_test%>%
  group_by(date)%>%
  summarise(test_vs_control=mean(conversion[test==1])/
                            mean(conversion[test==0]))

qplot(x=date,y=test_vs_control,data=data_test_by_day,geom="line")

ggplot(data_test_by_day,aes(x=date,test_vs_control))+
  geom_line()
```


From the plot we could tell that 
1. All of the test_vs_control are smaller than 1, meaning that Test has been constantly worse than control. There's relatively little variance across the days. That probably means that we do have enough data, but there was some bias in the experiment set up.

2. But there is this rising trend that could possibly exceed 1. And on a side note, the testing just ran for 5 days. Normally, it should always run at least 1 full week to capture weekly pattern, 2 weeks would be predicably better.


Likely, there is for some reason some segment of users more likely to end up in test or in control, this segment had a significantly above/ below conversion rate and this affected the overall results.

In an ideal world, the distribution of people in test and control for each segment should be the same.  
There are many ways to check this. 

One way is to build a decision tree where the variables are the user dimensions and the outcome variable is whether the user is in test or control. If the tree splits, it means that for given values of that variable you are more likely to end up in test or control. But this should be impossible! Therefore, if the randomization worked, the tree should not split at all(or at least not be able to separate the two classes well)

```{r}
tree<-rpart(test~.,data_test[,-8],control=rpart.control(minbucket = nrow(data_test)/100,maxdepth = 2))
# we remove conversion. Doesn't matter now.
# we only look for segments representing at least 1% of the populations.

tree # here we are not interested in predictive power, we are mainly using the tree as a descriptive stat tool
summary(tree)
```

Looks very interesting. The randomizaton is perfect for the countries on one side of the split(country=Bolivia,Chile,Colombia,Costa Rica,Ecuador,El Salvador,Guatemala,Honduras,Mexico,Nicaragua,Panama,Paraguay,Peru,Venezuela)
Indeed, in that leaf the test/control ratio is 0.498!

However, Argentina and Uruguary together have 80% test and 20% control! So let's check the test results after controlling for country.
That is, we check for each country how the test is doing:

```{r}
data_test_country<-data_test%>%
  group_by(country)%>%
  summarize(p_value=t.test(conversion[test==1],conversion[test==0])$p.value,
            conversion_test=t.test(conversion[test==1],conversion[test==0])$estimate[1],
            conversion_control=t.test(conversion[test==1],conversion[test==0])$estimate[2]
            )%>%
  arrange(p_value)

data_test_country
```


After we control for country, the test clearly appears non-significant. Nota great successgiven that the goal was to inprove conversion rate, but at least we know that a localized translation didn't make things worse!
